arch(n64_rsp) = R"(

// RSP CP2 Vector Operation Matrix Instructions (COP2):
vmulf v*01,v*05,v*05[e*01] ; %0100101 >>03d >>02d >>01d ~d ~c ~b >>04a >>03a >>02a >>01a ~a %000000
vmulu v*01,v*05,v*05[e*01] ; %0100101 >>03d >>02d >>01d ~d ~c ~b >>04a >>03a >>02a >>01a ~a %000001
vrndp v*01,v*05,v*05[e*01] ; %0100101 >>03d >>02d >>01d ~d ~c ~b >>04a >>03a >>02a >>01a ~a %000010
vmulq v*01,v*05,v*05[e*01] ; %0100101 >>03d >>02d >>01d ~d ~c ~b >>04a >>03a >>02a >>01a ~a %000011
vmudl v*01,v*05,v*05[e*01] ; %0100101 >>03d >>02d >>01d ~d ~c ~b >>04a >>03a >>02a >>01a ~a %000100
vmudm v*01,v*05,v*05[e*01] ; %0100101 >>03d >>02d >>01d ~d ~c ~b >>04a >>03a >>02a >>01a ~a %000101
vmudn v*01,v*05,v*05[e*01] ; %0100101 >>03d >>02d >>01d ~d ~c ~b >>04a >>03a >>02a >>01a ~a %000110
vmudh v*01,v*05,v*05[e*01] ; %0100101 >>03d >>02d >>01d ~d ~c ~b >>04a >>03a >>02a >>01a ~a %000111

vmacf v*01,v*05,v*05[e*01] ; %0100101 >>03d >>02d >>01d ~d ~c ~b >>04a >>03a >>02a >>01a ~a %001000
vmacu v*01,v*05,v*05[e*01] ; %0100101 >>03d >>02d >>01d ~d ~c ~b >>04a >>03a >>02a >>01a ~a %001001
vrndn v*01,v*05,v*05[e*01] ; %0100101 >>03d >>02d >>01d ~d ~c ~b >>04a >>03a >>02a >>01a ~a %001010
vmacq v*01,v*05,v*05[e*01] ; %0100101 >>03d >>02d >>01d ~d ~c ~b >>04a >>03a >>02a >>01a ~a %001011
vmadl v*01,v*05,v*05[e*01] ; %0100101 >>03d >>02d >>01d ~d ~c ~b >>04a >>03a >>02a >>01a ~a %001100
vmadm v*01,v*05,v*05[e*01] ; %0100101 >>03d >>02d >>01d ~d ~c ~b >>04a >>03a >>02a >>01a ~a %001101
vmadn v*01,v*05,v*05[e*01] ; %0100101 >>03d >>02d >>01d ~d ~c ~b >>04a >>03a >>02a >>01a ~a %001110
vmadh v*01,v*05,v*05[e*01] ; %0100101 >>03d >>02d >>01d ~d ~c ~b >>04a >>03a >>02a >>01a ~a %001111

vadd v*01,v*05,v*05[e*01]  ; %0100101 >>03d >>02d >>01d ~d ~c ~b >>04a >>03a >>02a >>01a ~a %010000
vsub v*01,v*05,v*05[e*01]  ; %0100101 >>03d >>02d >>01d ~d ~c ~b >>04a >>03a >>02a >>01a ~a %010001
vsut v*01,v*05,v*05[e*01]  ; %0100101 >>03d >>02d >>01d ~d ~c ~b >>04a >>03a >>02a >>01a ~a %010010
vabs v*01,v*05,v*05[e*01]  ; %0100101 >>03d >>02d >>01d ~d ~c ~b >>04a >>03a >>02a >>01a ~a %010011
vaddc v*01,v*05,v*05[e*01] ; %0100101 >>03d >>02d >>01d ~d ~c ~b >>04a >>03a >>02a >>01a ~a %010100
vsubc v*01,v*05,v*05[e*01] ; %0100101 >>03d >>02d >>01d ~d ~c ~b >>04a >>03a >>02a >>01a ~a %010101
vaddb v*01,v*05,v*05[e*01] ; %0100101 >>03d >>02d >>01d ~d ~c ~b >>04a >>03a >>02a >>01a ~a %010110
vsubb v*01,v*05,v*05[e*01] ; %0100101 >>03d >>02d >>01d ~d ~c ~b >>04a >>03a >>02a >>01a ~a %010111
vaccb v*01,v*05,v*05[e*01] ; %0100101 >>03d >>02d >>01d ~d ~c ~b >>04a >>03a >>02a >>01a ~a %011000
vsucb v*01,v*05,v*05[e*01] ; %0100101 >>03d >>02d >>01d ~d ~c ~b >>04a >>03a >>02a >>01a ~a %011001
vsad v*01,v*05,v*05[e*01]  ; %0100101 >>03d >>02d >>01d ~d ~c ~b >>04a >>03a >>02a >>01a ~a %011010
vsac v*01,v*05,v*05[e*01]  ; %0100101 >>03d >>02d >>01d ~d ~c ~b >>04a >>03a >>02a >>01a ~a %011011
vsum v*01,v*05,v*05[e*01]  ; %0100101 >>03d >>02d >>01d ~d ~c ~b >>04a >>03a >>02a >>01a ~a %011100
vsar v*01,v*05,v*05[e*01]  ; %0100101 >>03d >>02d >>01d ~d ~c ~b >>04a >>03a >>02a >>01a ~a %011101
vacc v*01,v*05,v*05[e*01]  ; %0100101 >>03d >>02d >>01d ~d ~c ~b >>04a >>03a >>02a >>01a ~a %011110
vsuc v*01,v*05,v*05[e*01]  ; %0100101 >>03d >>02d >>01d ~d ~c ~b >>04a >>03a >>02a >>01a ~a %011111

vlt v*01,v*05,v*05[e*01]   ; %0100101 >>03d >>02d >>01d ~d ~c ~b >>04a >>03a >>02a >>01a ~a %100000
veq v*01,v*05,v*05[e*01]   ; %0100101 >>03d >>02d >>01d ~d ~c ~b >>04a >>03a >>02a >>01a ~a %100001
vne v*01,v*05,v*05[e*01]   ; %0100101 >>03d >>02d >>01d ~d ~c ~b >>04a >>03a >>02a >>01a ~a %100010
vge v*01,v*05,v*05[e*01]   ; %0100101 >>03d >>02d >>01d ~d ~c ~b >>04a >>03a >>02a >>01a ~a %100011
vcl v*01,v*05,v*05[e*01]   ; %0100101 >>03d >>02d >>01d ~d ~c ~b >>04a >>03a >>02a >>01a ~a %100100
vch v*01,v*05,v*05[e*01]   ; %0100101 >>03d >>02d >>01d ~d ~c ~b >>04a >>03a >>02a >>01a ~a %100101
vcr v*01,v*05,v*05[e*01]   ; %0100101 >>03d >>02d >>01d ~d ~c ~b >>04a >>03a >>02a >>01a ~a %100110
vmrg v*01,v*05,v*05[e*01]  ; %0100101 >>03d >>02d >>01d ~d ~c ~b >>04a >>03a >>02a >>01a ~a %100111

vand v*01,v*05,v*05[e*01]  ; %0100101 >>03d >>02d >>01d ~d ~c ~b >>04a >>03a >>02a >>01a ~a %101000
vnand v*01,v*05,v*05[e*01] ; %0100101 >>03d >>02d >>01d ~d ~c ~b >>04a >>03a >>02a >>01a ~a %101001
vor v*01,v*05,v*05[e*01]   ; %0100101 >>03d >>02d >>01d ~d ~c ~b >>04a >>03a >>02a >>01a ~a %101010
vnor v*01,v*05,v*05[e*01]  ; %0100101 >>03d >>02d >>01d ~d ~c ~b >>04a >>03a >>02a >>01a ~a %101011
vxor v*01,v*05,v*05[e*01]  ; %0100101 >>03d >>02d >>01d ~d ~c ~b >>04a >>03a >>02a >>01a ~a %101100
vnxor v*01,v*05,v*05[e*01] ; %0100101 >>03d >>02d >>01d ~d ~c ~b >>04a >>03a >>02a >>01a ~a %101101
v056 v*01,v*05,v*05[e*01]  ; %0100101 >>03d >>02d >>01d ~d ~c ~b >>04a >>03a >>02a >>01a ~a %101110
v057 v*01,v*05,v*05[e*01]  ; %0100101 >>03d >>02d >>01d ~d ~c ~b >>04a >>03a >>02a >>01a ~a %101111

vrcp v*01,v*05,v*05[e*01]  ; %0100101 >>03d >>02d >>01d ~d ~c ~b >>04a >>03a >>02a >>01a ~a %110000
vrcpl v*01,v*05,v*05[e*01] ; %0100101 >>03d >>02d >>01d ~d ~c ~b >>04a >>03a >>02a >>01a ~a %110001
vrcph v*01,v*05,v*05[e*01] ; %0100101 >>03d >>02d >>01d ~d ~c ~b >>04a >>03a >>02a >>01a ~a %110010
vmov v*01,v*05,v*05[e*01]  ; %0100101 >>03d >>02d >>01d ~d ~c ~b >>04a >>03a >>02a >>01a ~a %110011
vrsq v*01,v*05,v*05[e*01]  ; %0100101 >>03d >>02d >>01d ~d ~c ~b >>04a >>03a >>02a >>01a ~a %110100
vrsql v*01,v*05,v*05[e*01] ; %0100101 >>03d >>02d >>01d ~d ~c ~b >>04a >>03a >>02a >>01a ~a %110101
vrsqh v*01,v*05,v*05[e*01] ; %0100101 >>03d >>02d >>01d ~d ~c ~b >>04a >>03a >>02a >>01a ~a %110110
vnop v*01,v*05,v*05[e*01]  ; %0100101 >>03d >>02d >>01d ~d ~c ~b >>04a >>03a >>02a >>01a ~a %110111

vextt v*01,v*05,v*05[e*01] ; %0100101 >>03d >>02d >>01d ~d ~c ~b >>04a >>03a >>02a >>01a ~a %111000
vextq v*01,v*05,v*05[e*01] ; %0100101 >>03d >>02d >>01d ~d ~c ~b >>04a >>03a >>02a >>01a ~a %111001
vextn v*01,v*05,v*05[e*01] ; %0100101 >>03d >>02d >>01d ~d ~c ~b >>04a >>03a >>02a >>01a ~a %111010
v073 v*01,v*05,v*05[e*01]  ; %0100101 >>03d >>02d >>01d ~d ~c ~b >>04a >>03a >>02a >>01a ~a %111011
vinst v*01,v*05,v*05[e*01] ; %0100101 >>03d >>02d >>01d ~d ~c ~b >>04a >>03a >>02a >>01a ~a %111100
vinsq v*01,v*05,v*05[e*01] ; %0100101 >>03d >>02d >>01d ~d ~c ~b >>04a >>03a >>02a >>01a ~a %111101
vinsn v*01,v*05,v*05[e*01] ; %0100101 >>03d >>02d >>01d ~d ~c ~b >>04a >>03a >>02a >>01a ~a %111110
vnull v*01,v*05,v*05[e*01] ; %0100101 >>03d >>02d >>01d ~d ~c ~b >>04a >>03a >>02a >>01a ~a %111111

// RSP CP2 Vector Load Instructions (LWC2):
lbv v*05[e*01],*06(r*01)  ; %110010 >>04d >>03d >>02d >>01d ~d ~a %00000 >>03b >>02b >>01b ~b %0 ~c
lsv v*05[e*01],*06(r*01)  ; %110010 >>04d >>03d >>02d >>01d ~d ~a %00001 >>03b >>02b >>01b ~b %0 ~c
llv v*05[e*01],*06(r*01)  ; %110010 >>04d >>03d >>02d >>01d ~d ~a %00010 >>03b >>02b >>01b ~b %0 ~c
ldv v*05[e*01],*06(r*01)  ; %110010 >>04d >>03d >>02d >>01d ~d ~a %00011 >>03b >>02b >>01b ~b %0 ~c
lqv v*05[e*01],*06(r*01)  ; %110010 >>04d >>03d >>02d >>01d ~d ~a %00100 >>03b >>02b >>01b ~b %0 ~c
lrv v*05[e*01],*06(r*01)  ; %110010 >>04d >>03d >>02d >>01d ~d ~a %00101 >>03b >>02b >>01b ~b %0 ~c
lpv v*05[e*01],*06(r*01)  ; %110010 >>04d >>03d >>02d >>01d ~d ~a %00110 >>03b >>02b >>01b ~b %0 ~c
luv v*05[e*01],*06(r*01)  ; %110010 >>04d >>03d >>02d >>01d ~d ~a %00111 >>03b >>02b >>01b ~b %0 ~c
lhv v*05[e*01],*06(r*01)  ; %110010 >>04d >>03d >>02d >>01d ~d ~a %01000 >>03b >>02b >>01b ~b %0 ~c
lfv v*05[e*01],*06(r*01)  ; %110010 >>04d >>03d >>02d >>01d ~d ~a %01001 >>03b >>02b >>01b ~b %0 ~c
ltwv v*05[e*01],*06(r*01) ; %110010 >>04d >>03d >>02d >>01d ~d ~a %01010 >>03b >>02b >>01b ~b %0 ~c
ltv v*05[e*01],*06(r*01)  ; %110010 >>04d >>03d >>02d >>01d ~d ~a %01011 >>03b >>02b >>01b ~b %0 ~c

// RSP CP2 Vector Store Instructions (SWC2):
sbv v*05[e*01],*06(r*01) ; %111010 >>04d >>03d >>02d >>01d ~d ~a %00000 >>03b >>02b >>01b ~b %0 ~c
ssv v*05[e*01],*06(r*01) ; %111010 >>04d >>03d >>02d >>01d ~d ~a %00001 >>03b >>02b >>01b ~b %0 ~c
slv v*05[e*01],*06(r*01) ; %111010 >>04d >>03d >>02d >>01d ~d ~a %00010 >>03b >>02b >>01b ~b %0 ~c
sdv v*05[e*01],*06(r*01) ; %111010 >>04d >>03d >>02d >>01d ~d ~a %00011 >>03b >>02b >>01b ~b %0 ~c
sqv v*05[e*01],*06(r*01) ; %111010 >>04d >>03d >>02d >>01d ~d ~a %00100 >>03b >>02b >>01b ~b %0 ~c
srv v*05[e*01],*06(r*01) ; %111010 >>04d >>03d >>02d >>01d ~d ~a %00101 >>03b >>02b >>01b ~b %0 ~c
spv v*05[e*01],*06(r*01) ; %111010 >>04d >>03d >>02d >>01d ~d ~a %00110 >>03b >>02b >>01b ~b %0 ~c
suv v*05[e*01],*06(r*01) ; %111010 >>04d >>03d >>02d >>01d ~d ~a %00111 >>03b >>02b >>01b ~b %0 ~c
shv v*05[e*01],*06(r*01) ; %111010 >>04d >>03d >>02d >>01d ~d ~a %01000 >>03b >>02b >>01b ~b %0 ~c
sfv v*05[e*01],*06(r*01) ; %111010 >>04d >>03d >>02d >>01d ~d ~a %01001 >>03b >>02b >>01b ~b %0 ~c
swv v*05[e*01],*06(r*01) ; %111010 >>04d >>03d >>02d >>01d ~d ~a %01010 >>03b >>02b >>01b ~b %0 ~c
stv v*05[e*01],*06(r*01) ; %111010 >>04d >>03d >>02d >>01d ~d ~a %01011 >>03b >>02b >>01b ~b %0 ~c

)";